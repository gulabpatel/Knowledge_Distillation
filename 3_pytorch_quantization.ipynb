{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+uxiW4KILfoTF/Ug8kUS2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulabpatel/Knowledge_Distillation/blob/main/3_pytorch_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calibration\n",
        "The process of choosing the input clipping range is known as calibration. The simplest technique (also the default in PyTorch) is to record the running mininmum and maximum values and assign them to  and . TensorRT also uses entropy minimization (KL divergence), mean-square-error minimization, or percentiles of the input range.\n"
      ],
      "metadata": {
        "id": "3AH2w6VNYhQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "19Vm3QPSZrqq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WQnAp6_WjMG",
        "outputId": "d621e229-7d55-4cbc-b8da-5046d3753698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[-0.7263, -1.4862,  0.4027,  0.2487],\n",
            "        [-0.7735,  0.5579, -0.1400,  0.5003],\n",
            "        [-0.8770, -1.8155, -0.1084, -0.5743]]), tensor([[-0.7071,  1.5024, -1.3229, -0.8100],\n",
            "        [ 0.5470,  1.1638, -0.5730, -0.4987],\n",
            "        [-0.5997, -1.4349,  0.0804,  1.1246]])]\n"
          ]
        }
      ],
      "source": [
        "from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n",
        "C, L = 3, 4\n",
        "normal = torch.distributions.normal.Normal(0,1)\n",
        "inputs = [normal.sample((C, L)), normal.sample((C, L))]\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "observers = [MinMaxObserver(), MovingAverageMinMaxObserver(), HistogramObserver()]\n",
        "for obs in observers:\n",
        "  for x in inputs:\n",
        "     obs(x) \n",
        "  print(obs.__class__.__name__, obs.calculate_qparams())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mgc9Mab1ZnIp",
        "outputId": "1e66a13a-7ca3-457d-9c6e-bdcd6989a7cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MinMaxObserver (tensor([0.0130]), tensor([140], dtype=torch.int32))\n",
            "MovingAverageMinMaxObserver (tensor([0.0093]), tensor([194], dtype=torch.int32))\n",
            "HistogramObserver (tensor([0.0101]), tensor([180], dtype=torch.int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Affine and Symmetric Quantization Schemes"
      ],
      "metadata": {
        "id": "CE-wfG0PaHVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "act =  torch.distributions.pareto.Pareto(1, 10).sample((1,1024))\n",
        "weights = torch.distributions.normal.Normal(0, 0.12).sample((3, 64, 7, 7)).flatten()"
      ],
      "metadata": {
        "id": "Gxda6qTPZ7MW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_symmetric_range(x):\n",
        "  beta = torch.max(x.max(), x.min().abs())\n",
        "  return -beta.item(), beta.item()\n",
        "\n",
        "def get_affine_range(x):\n",
        "  return x.min().item(), x.max().item()\n",
        "\n",
        "def plot(plt, data, scheme):\n",
        "  boundaries = get_affine_range(data) if scheme == 'affine' else get_symmetric_range(data)\n",
        "  a, _, _ = plt.hist(data, density=True, bins=100)\n",
        "  ymin, ymax = np.quantile(a[a>0], [0.25, 0.95])\n",
        "  plt.vlines(x=boundaries, ls='--', colors='purple', ymin=ymin, ymax=ymax)\n",
        "\n",
        "fig, axs = plt.subplots(2,2)\n",
        "plot(axs[0, 0], act, 'affine')\n",
        "axs[0, 0].set_title(\"Activation, Affine-Quantized\")\n",
        "\n",
        "plot(axs[0, 1], act, 'symmetric')\n",
        "axs[0, 1].set_title(\"Activation, Symmetric-Quantized\")\n",
        "\n",
        "plot(axs[1, 0], weights, 'affine')\n",
        "axs[1, 0].set_title(\"Weights, Affine-Quantized\")\n",
        "\n",
        "plot(axs[1, 1], weights, 'symmetric')\n",
        "axs[1, 1].set_title(\"Weights, Symmetric-Quantized\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "roa61ec_ahis",
        "outputId": "c0c78311-c876-4993-c05e-41f7a9740226"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEICAYAAACuxNj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de7xUVd3/3x8BL4RiCokX8GgYpfYLDS171CytvOWlzDQzzQqt7NHn0Z7MzEfN1C7mJTM1NSw1JQvznnfJWwI+WCBhiCgooKCCqKjk9/fHWhP7DDNnZs7ZM3vvme/79ZrXmb3X2mt999qftb7rNvvIzHAcx3GcnlgtawMcx3Gc/OPOwnEcx6mJOwvHcRynJu4sHMdxnJq4s3Acx3Fq4s7CcRzHqYk7C8fpI5IukvT9JqU9XdIuzUi7U5B0q6TDsrYjDSSNkLRMUr+U050jabee4hTGWXRChZS0v6S5UQzbSBolaaqkVyT9ZzPLII9IOlHSpSmnuYukeYnjeyW9JGmNOq8/XNL9yXNmdpSZ/SAF28ZJOr0s7a3M7N6+pl1H3vtGrS2VtEjS3ZI2a3a+fUHSKZKurBXPzPYwsyt6mYckfVvSPyW9LukZSWdIWr036fUi/26NuJk9Y2aDzOxfrcg/Sf9WZCLpXuADwDAze6OO+IcDXzWzHUvnzOyolGwZB8wzs5MSaW+VRtp15i/gSWC5mW1ZFvxT4Ggz+1OMexlwj5mNbpFtmwA/AnYH1gSmA6eY2S0tyHsX4Eoz26R0zszOaHKeXcBOwBJgH+D3zcwvr0gaCfwG+AxwNzAI+CTQ8gYpTWJdk5m93YdkzifUhy8Bk4BRwK+B9xLKq3Mws6Z+gC6C6F4EPlfnNYcD9zfJnnHA6c2+7x7y/yiwDFgObFcWtgIYmTi+k+A0W2HXesAcQkUYBqwFHAwsBfZrQf67EJx4y/IBTgYeAH4G3FQWbzjwR+AFYDFwAfC++Nz+FZ/hy+WaAmYAeyfS6R/T2DYe/x5YQHBQE4Gt4vmxwFvAmzHtG+P5OcBu8fsawLnAc/FzLrBG8r6A44DngfnAl+sskwOAqVXChgGvAesnzm0b72lArKsPAOcALwOzgY/E83OjLYclrh0HXAjcGu/zgZjHucBLwD+AbRLxNwL+EPN7CvjPeH73WFZvxXQei+fvBX4Y030dGBnPfTWR5tfic3oFeLz0bCrc+xbxWW9fQRtvAB9N5JlM/3AS7RdwXiyLpcAUYKdE2CnAeIKzfoXQQRsTw34LvB3vYxnwP4T21Ai62iGeL32WA3PitasBJxA6potjHusl8j0UeDqGfY+EzqrqpAWV0ytk93u+HLgq3vcFiTyXRRG8Gh/w3bEMlsew95SVQY+2xDR/CjwDLAQuAtbqwa4fANOA1crOf4fQACgp1ET4vcSKArw72r0YWBTvc91E3DnA8cDf4rO5ljCCeQehQrzNSuFvRKhIV8ZrL6B7xVhBGPVAlQYlhq0Vy+0lQsPwbVY6i1nAN4APRl1sEM/3Ax4jNIDviDbuWKkhqKDNk4GrEmF7ATMSx0cAa7NSZ1MrpVNWZiVtngY8DLwLGAo8CPwgoYcVMc4AYE9CI//OOjS5OUFn5wAfAwaVhd8CfD1xfA7w80R5rAC+HMvtdILmfhHv8ZOERnBQ4h4XxTJfk6CXpwg999L19yQavCmxTFePds4GPpVoaK8ss/XemP9WhHZhAN01+jngWWA7gqZHAptWKZejgKerhN0H/LC8DlTSCPBFYP1oz3GEtmnNxD0sj8+rH3Am8HCl5x+Puyirg/H8gGjTmfH4mKiVTeJzuBj4XQzbklCHdo5hP4vPMHNn4RVyZT4DCb2LPYHPEirN6olwo/vIolyEyTLo0ZZYrjcQRgxrAzeWhFTFtoeBUyuc3yzatUUlodK9Io4EPhHLfSjBUZ9bVs6PEBr39QhO/6jE/cwry/sUyhqDeH40wTFsQ+0G5SzgLzG/4QSHOA/YkaDHITHeP4D/it93iOn3r5D34fSszZGExnFgPL4KOLlKma8by3Nwndp8EtgzEfYpVvYkdyE43OSzeR74cJ3a/DCh9/kCofEax8oG/vPAA4l6u4DY247l8c9EOu+P97RB4txiYHTiHn+VCPsW3evu+1nZQfwQ8EyZnd8Ffl1NHwQ9nlbhXEmjfwaOqbNMTiLRcJeFXQNcUqWerqKRsmtfAj6QuIc7E2FbAq9Xev7xuIvKzuKXwE3Ezh6hbu2aCN+QoPf+hLpyTSLsHYQOdI/OoqkL3JJ2BDYFxpvZFILYvxCDtyc0Gt82s1fNbLmZ3V8lqXKuBvaRNDAefwH4XSnQzC43s1csrI+cAnxA0uA60z6EILbnzewF4FTCkK3EWzH8LQtz+csI85j18BnC8PV24GZCI79XnddWoqItca52LKHxe9HMXgHOAA7qIa0hhNFJOaVzQ2sZY2azzOwOM3sjlt3PCNNuSc43s+fM7EWCA2toPUbSUOB64Ftm9n+EHuJQMzvNzN40s9nAr1h5rwcSeoAvmtlcwhw0wGHA7Wa2KB5fHc9BcCpPm9mKRmyDUAaEivrpqM99YtpI6ifpLElPSlpKaAgglH09bESYOijxdDxXYnGZza8R1h/qsfthMzvQzIYS1nF2JkxPAPwJ2DIueH8CWGJmjyQuX5j4/npMr/zcoB7iV4u7KbCRpJdLH+BEYIMatzO3h7DhhHaoG5IOiRtLlkm6NZ5eRGhkK7FhDK+JpOMlzZC0JN7DYLo/8wWJ768Ba0qqez1Z0pGEzsIXbOX6zKbAhES5zSDMVGxA0My/y8jMXiU49B5p9gJ3tQp5Dn2skJJKFfJGQoXcBkKFJMxZfo7QwJUKbwhh6qMWTauQhHsfH69fIekP8dyEOq8vp5otQwmjmCnBbwBhyN0PwlZCQoMAcKSZXUX1ilE6V7NiSNqAMD+7E2E0sxqhF5WkvGJsRJ1IGgBcB1xtZtfE0/9uUBJR+xFGE1BWMVj5bA8E+kkq2bMGsK6kD8T4IyT1r6BPq8PU3xHWe1YDHo8OBEKnZl9gN4KjGEwon9JDqpX2c4T7nR6PR8RzqWJmkyT9Edg6Hi+XNJ4wnfJewlx6K5gLPGVmW1QJr1ZePZXjXMJ0afcLQh24quz03cCFkrZPOkdJwwkjsR/GU68S6luJYYm4OxHWGnYFppvZ25KSz7wWPWoipv8DwqzM0kTQXOAIM3ugwjXzCdP9peOBhGmyHmnayELSWoQK+VFJC2Kl/C9CL79bhaxweSMVcl+qV8jBhGEbNF4hS6RSIeNOo48DX0yUxwHAnpLq7VnWyyJCD20rM1s3fgab2SD491bCQfFTqiB3Ap+RVK6JAwnTNrMIlQKqVAzC6MWA95vZOoTGJZVKEfk5YRrvpMS5UoOybuKztpntGcPnEzomJUYQ1jH+RRjyj46f9xEczJcIU2XzgbMkvUPSmpL+I16/ENikxtbJawhz9V8njioiaxNGlosJZVi+22shYRqtGr8DTpI0NGrmZKDm1lH495bfOVXCdpT0NUnvisfvJXTAHk5E+w1hemUfWucsHgFekfQdSWvFkdnWkraL4QuBrgqa7YlLgeMlfTBuix0padNKEc3sCcJa31WSPhzz34qwPvYgoc4ATCXUnYFxZ9lXEsmsTZgufgHoL+lkYJ0G7K2qiei0xgNfirYmuQj4Yeneomb2jWHXAXvH5746YSq7Zhk2cxpqP7xCJjkUeIIwZVUqj/cQGuKD60m/XuJQ9FfAOYkGYGNJn+rhsnMIzvUyScPi8zgY+D7wv2b2dpxaepbg8PpJOoLuvbS1CVNhSyRtTFhMrpeFwPrVpgvjUPujwCHWfStkrQZlPPBdSe+MDvtbhDnaX1vYs76g9CEsoh9CcHCfJqw/PEN4Rp+P6d1N6NkvkFRxtGVm84GHCLuCrk0E/YYwsnmWsNj+cNmllxGme16WdH2FpE8HJhM2CPwdeDSeq4fhhI0mlXiZ4AT+LmkZcBthtPvjxD09QBilP2pmT1dMJWUs/JZgb0JdeYrQCbqUoFNYudV5saRH60zz94QRwdWEtaXrCetZ1Tg65nklYSQ8jfAM90vo8BzCnP9C4Aq6j1D+TCjPJ+J1y+l5mqycMwnt0cuSji8L25UwrXRdYgqtNOo8j7BmebukVwha+1Asg+nAN2MZzCeMbudRi54WNPryiQV0doXzBxKmIvoTennXs3L3zPkxzuqEOf0XgUVWffHvLoLXHpY4N4gwx/oK4eF8icTCMWGhdiqhglxvqy4irkmY154fP+ezcufCLqy6CJu89vskFt7L4v2DMM9efv5/gMnxe6ML3D3ZsibBUc4m9MZnkNglVMXGEQRn+WIs17dIbHuMcfYgVNyXgbMJOzBKi4dbERabl8UyPi5pI6su1p1CYoGSsFNscUy7fDfUvYROQHJH1IkxbKNo9wKC8B9OlMNAQiP9MmW7oTrpQ1gne18f07ibFm3lzuuHsIb5NxK7/Drlo1gATgpIup2w02JG1rb0FUnrEHqiE8zs5KztcbIljtTuAIZb2DDRsUg6GphlZrdlbUsrcWfhVCXOiR4BXGxhmsbpQCRdQZhWPsbMxmVsjpMR7iwcx3GcmhTmRYKO4zhOdrTkRYKVGDJkiHV1dWWVvdPmTJkyZZGFH5i1HNe200yy0nZmzqKrq4vJkydnlb3T5khqyfbOSri2nWaSlbZ9GspxHMepiTsLx3EcpybuLBzHcZya5NpZdJ1wc9YmOI7T5ng7Ux+5dhaO4zhOPnBn4TiO49Qks62zPeHDQsdxnHzhIwvHcRynJu4sHMdxnJq4s3Acx3Fq4s7CcRzHqYk7C8dxHKcm7iwcx3GcmrizcBzHcWrizsJxHMepSU1nIWmUpKmJz1JJx5bF2UXSkkSck5tnsuOkg2vbceqn5i+4zWwmMBpAUj/gWWBChah/MbO90zXPcZqHa9tx6qfRaahdgSfNLLP/QuY4TcK17Tg90KizOAj4XZWwHSQ9JulWSVtViiBprKTJkia/8MILDWbtOE3Fte04PVC3s5C0OrAP8PsKwY8Cm5rZB4CfA9dXSsPMLjGzMWY2ZujQlv+/ccepiGvbcWrTyMhiD+BRM1tYHmBmS81sWfx+CzBA0pCUbHScZuPadpwaNOIsDqbKMF3SMEmK37eP6S7uu3mO0xJc245Tg7qchaR3AJ8A/pg4d5Sko+LhAcA0SY8B5wMHmZmlZWTXCTf7/7hwmkLW2nbygbcvtanrnx+Z2avA+mXnLkp8vwC4IF3THKf5uLYdpz78F9yO4zhOTXLnLJbMXcI7lr6ZtRlOzlkydwlL5i7J2oyGKKLNTuvJq05y5ywmHDqBnW6a/e9jn0t0KjHh0AlMOLTSj63zSxFtdlpPXnWSO2fhOI7j5A93Fo7jOE5N3Fk4juM4NXFn4TiO49Skrt9ZtJIdjtuBy66YnLUZTs7Z4bgdsjahYYpos9N68qqT3DmLUZ8exdwHZmVthpNzRn16VNYmNEwRbXZaT151krtpqEUzF7HO4uVZm+HknEUzF7Fo5qKszWiIItrstJ686iR3zuKmI2/iI3+ek7UZTs656cibuOnIm7I2oyGKaLPTevKqk9w5i3rwFws6jpM23qb0TKGchT9Mx3GcbCiUs3Acx3GywZ2F4ziOU5PcbZ3d+aSdufjSv2ZthpNzdj5p56xNaJgi2uy0nrzqJHfOYvPdNmf+nTOyNsPJOZvvtnnWJjRMEW12Wk9edVLvv1WdI+nvkqZKWuXn1QqcL2mWpL9J2ra3Bi2YuoD1Fr7W28udDmHB1AUsmLqgz+m0Wttp2Oy0N3nVSSNrFh8zs9FmNqZC2B7AFvEzFvhlbw267djb2P6uZ3p7udMh3Hbsbdx27G1pJdcybados9Om5FUnaS1w7wv8xgIPA+tK2jCltB0nS1zbjkP9zsKA2yVNkTS2QvjGwNzE8bx4znHyjmvbceqg3gXuHc3sWUnvAu6Q9A8zm9hoZrEyjgUYMWJEo5c7TjNwbTtOHdQ1sjCzZ+Pf54EJwPZlUZ4FhieON4nnytO5xMzGmNmYoUOH9s5ix0kR17bj1EfNkYWkdwCrmdkr8fsngdPKot0AHC3pGuBDwBIzm98bg3Y9Y1cuvPDB3lzqdBC7nrFrn9PIQtuOU4u86qSeaagNgAmSSvGvNrPbJB0FYGYXAbcAewKzgNeAL/fWoOEfGc7zNwzq7eVOhzD8I8NrR6pNy7XtOLXIq05qOgszmw18oML5ixLfDfhmGgbNfXAu75q3jOc3cYfhVGfug2HNuS8VKwttQ34bAycf5FUnuXs31F0n3sW2E+dlbYaTc+468S7uOvGurM1oiCLa7LSevOokd86iFv6acsdxnNZTOGfhOI7jtB53Fo7jOE5N3Fk4juM4NcndK8p3P3d3zj/vL3XF7TrhZuactVeTLXLyyO7n7p61CQ1TRJud1pNXneTOWQwbPYwXNxiYtRlOzhk2eljWJjRMEW12Wk9edZK7aajZd85mwzlLszbDyTmz75zN7DtnZ21GQxTRZqf15FUnuRtZTDx9Ih+YvZj5XetkbYqTYyaeHt71l9f/KlaJItrstJ686iR3IwvHcRwnf7izcBzHcWrizsJxHMepiTsLx3Ecpya5W+De++K9+dlP72v4Ov/NRWex98V7Z21CwxTR5k4jD+1IXnWSO2cxZNQQlq6/Zipp5eHBO81hyKghWZvQMEW02Wk9edVJ7qahZt44k+GzXs7aDCfnzLxxJjNvnJm1GQ1RRJud1pNXneTOWTx09kNs9ciCXl/fdcLN/hrzDuChsx/iobMfytqMhiiizZ1I1u1HXnWSO2fhOI7j5I+azkLScEn3SHpc0nRJx1SIs4ukJZKmxs/JzTHXcdLDte049VPPAvcK4Dgze1TS2sAUSXeY2eNl8f5iZi1fxs96yOgUmlxr23HyRM2RhZnNN7NH4/dXgBnAxs02zHGajWvbceqnoa2zkrqAbYC/VgjeQdJjwHPA8WY2vcL1Y4GxACNGjKiYx/6/3Z+fnHl3I2Y5Hcj+v90/1fRapW3HqUVedVK3s5A0CPgDcKyZlb9D/FFgUzNbJmlP4Hpgi/I0zOwS4BKAMWPGWKV8Bg8fzKvrrF6vWU6HMnj44NTSaqW2HacWedVJXbuhJA0gVKarzOyP5eFmttTMlsXvtwADJPXqlyXTrp3GZjNe7M2lTgcx7dppTLt2Wp/TabW207DZaW/yqpN6dkMJuAyYYWY/qxJnWIyHpO1juot7Y9DkX05m1P8935tLfbG7g5j8y8lM/uXkPqWRhbb7arPT/uRVJ/VMQ/0HcCjwd0lT47kTgREAZnYRcADwdUkrgNeBg8ys4lC8VbjjcOqgkNp2nCyo6SzM7H5ANeJcAFyQllGO0wpc245TP23/C25//YfjOI3ibcaqtL2zcBzHcfpO7l5RfuB1B3LmabdnbYaTcw687sCsTWiYItrstJ686iR3zmLgkIG8MXBA6ukmh5X+Py6Kz8AhA7M2oWGKaLPTevKqk9xNQ00dN5WRf1+UtRlOzpk6bipTx02tHTFHFNFmp/XkVScd6Sx88ar45LVC9UQRbe5ksmon8qqT3DmLVuEOw3Ecp3461lk4juM49ePOwnEcx6mJO4syfHrKcZwk3iYEcrd19pBbDuEH3781azOcnHPILYdkbULDFNFmp/XkVSe5G1kMGDiAfw3o19I8vedQPAYMHMCAJvwep5kU0Wan9eRVJ7lzFpMunMR7H+3dK8rTpKd3Svn7prJn0oWTmHThpKzNaIgi2uy0nrzqJHfTUNPHT6dr9ov8Y9t3tTRfb/yLxfTx4T+bbveN7TK2pH6KaLPTevKqk9yNLBzHcZz80dHOotp0UvKcTzk5TudSqvveBnS4s2iEepyGC8px2p9Ored1rVlI2h04D+gHXGpmZ5WFrwH8Bvgg4f8Tf97M5qRrav7oVNG0E65tpxE6uc6r1r8TltQPeAL4BDAPmAQcbGaPJ+J8A/h/ZnaUpIOA/c3s8z2lO2bMGJs8ufI/JW+nB1J6HXrXCTcz56y9ery38rglyo9rUcqjVa9ib9S+ViBpipmNqRGn5dp28kca7U2pbreiHtSj7WZQzzTU9sAsM5ttZm8C1wD7lsXZF7gifr8O2FVSj//buFNITl/VM42VjFtrPSUZr5647URK9+badlKhfJ2zHalnGmpjYG7ieB7woWpxzGyFpCXA+kC3d41LGguMjYfLJM2skueQ8mvbnKr3qx91/1v+vVLcWueaQS/y6fMzrpHnpnUkkYW2y8mD1rO2oW3yr1Rfm5B/PdpOnZb+zsLMLgEuqRVP0uQshllZ0Wn3C+13z/Vqu5w8lEPWNnj+2WugHuqZhnoWGJ443iSeqxhHUn9gMGEx0HHyjGvbceqkHmcxCdhC0maSVgcOAm4oi3MDcFj8fgBwt9VaOXec7HFtO06d1JyGivO0RwN/JmwvvNzMpks6DZhsZjcAlwG/lTQLeJFQ6fpCw8P5gtNp9ws5uOeMtF1O5uVA9jZ4/gWg5tbZPCPpEOAwM/tkHXEPB75qZjs23bDatuwPnA+8E9gJeA24Fng38D1gS+BZM/tBZka2EEknApub2VdTTHMX4Eoz2yStNBvMv5DaLDKSbgWuMbMrakbOOZJGAI8Dg83sXymmO4egtTsbvbblv+CW9N34UJPn/lnlXI+9ODO7qp7KWKdd90pKs7GSpNmSHq8Q/FPgaDMbZGb/B/wPcI+ZrW1m55vZUc10FJI2kXSVpMWSXpX0iKQ9m5VfWd67SJqXPGdmZ6TpKHpLJ2hT0r6SpkpaKmmRpLslbZZG2s1C0imSrqwVz8z26K2jiPX12/HZvi7pGUlnxOnJpiNpjqTdSsdm9kxsH1JzFH0li9d9TAQ+En8QhaQNgQHANmXnRsa4RWVn4F3A5pLKXx+5KTC9h+OmIWk94H7gTWArwra9c4BrJO3XChtyTFtrU9JIwq/RjyMs1G8G/ALITYPUG2JD39e27HzC1ucvAWsDewC7EX574wCYWUs/wOqEaZc/Ac8DzwC/Bu4DPhjjHAjMIjzA2YS54hcIO1NOB/rFeIcD9yfS/iQwE1gCXBjT/GoyLqFX/xLwFLBHDPshocIsB5YBFwAiNKLPA0uBvwNbN3CflwNXAX8ELojnxgFvAwa8CjwJ3J3I+3XglXi/C4CTgV0I+/+Pi7bMB76cyGeNeE/PAAuBi4C1erDrB8A0YLWy89+JZS2gK9rYPxF+b6Is3x3tXkzYH34VsG4i7pyYz7J4b0ui/e+I9/h2DFtGWBN4MT6TbWPZL0t8VgCnxHQ3Av4QtfAU8J+JPNeK5fsSYfj+bWBeL7WZ1GFFbcbvg6P98+mbNm+J+b5AcOJP0wRtEhbop1Y4/7lomwG7Js5vG20aEO/ngZjvy1ErH4nn50ZbDktcOy7e563R7geAYcC58Rn9A/hmzHdWvM9Vni2weyyTt2I6jyX0+MOY7usEB35vqUxjnK8BMwh16nFg2wp19HnCr/j/BWxfFj4ceAP4aHkdqPKMz4tlsRSYAuyUCDsFGE9w1q8QOod7AffE8rR4n8sIMw1d8Vx/YAe614nlwJyY7mrACYS2ZHHMY71EvocS9LSYMMU9B9itV213s5xCDdHeA/w8inExcER88P8Vwy8Abo9CmxA/kwg99UeAI8sfFqGHvBT4TCzgY6LAks7irSigfsDXgedYuW5TLoRPxQe+LqFyvg/YsM77Gxht2RP4LKFBXZ0w2tg2imBkeUNMcAw3ESra6TFsF0KDeRqh0u5JaFjeGcPPIezYWY/QI7oROLMH2x4GTq1wfrNo1xbUdhYjCa/IWAMYSuhln5uIOwf4P0IDuR6hQVhIWIvZhdiIx3u5lVCRbgP+WmbTaELjsQ2hUkwhONDVgc0JDdanYtyzgL/E/IYTHGJDziKhzaQOK2nz8vh9AnAxwQn2RZs/YaU2f0xYcE9dm7HMlkfNfAwYFM+/DxhFqItnJuKfA/w8cT8rgC8T6s/phA7KL6IOPkloBEtpjiPo/oPAmoTOxVOEnnu/WKavR5vWIGj651We7SmE9afkvdwb898qlukAumv0cwQHvl0so5HApmVplOrjs8DTVcrsPuCHVZ7Dv59xPP4i4Qeb/QmdowXAmol7WE7QfD/gzPgMt43hTxM6hVvG4y7K6mA8PyDadGY8PoZQpzeJ5Xgx8LsYtiXBuewcw34Wn2GvnEVWb529j3BzLxIa1r/Ez84xfCdCofyJULhfANYhFPI5VN6Rsicw3cz+aGYrCKOSBWVxnjazX1mYB7wC2BDYoIqNbxEa3/cSKu0MM5tf5/19htAjuR24Od7LXmY2Md5zo7wFnGZmb5nZLQQBjIqvnRhLaMheNLNXgDPoecfOEEJPuJzSuaG1jDGzWWZ2h5m9YWYvEET40bJoZ5vZ7Wb2IuE5vkb4NXSSfQk9LQgNy7pxmgdJQ4HrgW9ZWNfZDhhqZqeZ2ZtmNhv4VeJeDyRU6hfNbC7h+feG++iuw0ravE/SBgTNHWtmr5rZ8/Rem9OJ2gQeIvS8U9dmLLNdCM9hPLBI0jhgrpnNjDbtAf9+b9bBwG8TSTxlZr+O9edaglM+LergdkLPeGQi/gQzm2JmywmOdbmZ/SZe/zihIZxN6BS8CTxX5dlWY5yZTTezFWb2VlnYV4Efm9kkC8wys6fLyqNUH/tTuU4Qz9esEzG9K81scbTnbEIDPSoR5X4zuyXe/28JjuHR0uUEh1FeR8o5n+CUvxePjwK+Z2bzzOwNglM6IP4m6ADgJjObGMO+TxjV94qsnMVEYEfCML6/mf0TeJAwX7wesDXhpv5FaGjnE3obTxA8Z6V/o7cRiVc3WHCt88riLEiEvxa/DqpkoJndTehF/gJ4XtIlktap8/4OA8ZH0SwnDK8Pq3FNiR2AfYCDJG0Vzy2OjUyJ16LdQwnOdoqklyW9TOihD4WwO0TSsvgp/Rf4RYSGqJwNE+E9ImkDSddIelbSUuBKghNKkmwMVyc8s7+Wxan0uo2NJQ0gvIfpajMrzRlvCmxUus94ryeyskHdqCytbg1DA0wEdow6HFpFmxOjPQOA+Ql70tDmEYTRJTRBm2b2sJkdaGZDCY5vZxoLGNgAABO4SURBVFY2PIsJa2ybEUaOS8zskcTlCxPfX4/plZ8b1EP85PE7CZ0/CGW5NnBKlWdbjbk9hA0nTM10Q9IhiTpR2riwgsp1gni+rldxSDpe0gxJS+I9DKZ7vUjWideANWOjDsFhjWTVOpJM/0iCs/+CmZUa/U2BCYlym0FoNzdgVd29Sh9+UJrVv1V9iFCQBxMKDTNbKuk5wlD8OYK4nif00IcQhubfMbNqr/OcTxitAGHRK3lcB6vsITaz84HzJb2L0BP7NsE7V0XSJsDHge0lfTaeHkgQRnmDWs6jhId/AaFXcj2hPKqxiFBOW5lZ+S+PMbM9KlxzJ/AZSacmBAehZz6PMH+8fsLupfH7sETcMwjl9X4zezEujF9QyUBJg2Lak+Mzrmev9s9jviclzs0l9Gy3qHLNfEIDUdooMKKOfCpR0ubXCPPhq2jTzJ6StJyozTJHXs22T0qaljg3Chgh6YXSCUnfIzRcV9G9R99nbUq6k+7PsMRjBAcIoYN2B2E65b1lNjSTuYTpxuvM7OgK4dU005OW5hLW1rpfYHYVoXwBkNRFWD8cKWn7pHOUNBz4MGHKjBhvYCK5YYm4OxHWGnYljCLflvQSYQqsR2IdGUqYWlpaJc5OhHXAHcvizAWOMLMHKlwznzDFWDoeyMq63TCZjCzM7HVgMvAVwgMocT/w34Se27OEB3M7cDah8j8n6d2Syqc8IEz3vF/SftFbf5PKlaMaCwmjFwAkbSfpQ7GX+yphvvHtGHa4wn7lShxKGAGNIgyvRwPvITTEB/dkgJktNbNl8fBJQs91cA/x3yYM18+JjQaSNpb0qR6yOSemeZmkYZLWlHQwoaH5XzN7O04tPQt8UVI/SUfQveKtTZgKWyJpY0JDtQqx7P5AWIAt9XAWAutLGkzl1218jDCldUiZM3sEeEXSdyStFe3aOrHTbDzwXUnvjA77Wz2UQVUS2vxvwvRTiaQ2idM+twNnS1pH0mo1tNmP4PxGEzYhQCjvP8Xv7wL2jvdd3gj2WZtmththyuI84ONmtjVhmmIrwpx3iVsIc/H70FxnkexlP0LoDb+7yrNdCHQ1uOPpUuB4SR+Mu6VGStq0Stw3Cc/kKkkfjvlvRdDug4QOFsBUQkdroMLOsq8k0lib4OhfAPpLOpkwdV6L/jGf+YRO8ipEpzUe+JKZPVEWfBHww9K9SRoqqfTm5OuAvSXtqLAF+DT60OZn+Z/y7iOMGF5LnPsLodJMJCzafil+NiH0uGcQCmCVIaOZLSIsav2YMNTaklDp36jTnvMIc30vSTqf8KB/RZg/Lu0m+EmMO5zY66zAYcCFZrYg+SE81B6nomLjXeqJbER4Pktq2P0dwmjg4TgldCfd50m7YWaLCVOAaxLmjZcR1g2+aWaXJ6J+jeAEFhMalAcTYacSFgaXEBrCP1bJ7jLCM3sokf8/gN8RFjAPYeXIaUhMby9Cw/hcYrrgxDjPuzehsX2KMKq6lJXO9FTCc3qK0Ij3paG7j6DD+xPnktos8SXCFNvjBJ30VpvvJ2h8n8T0aJK0tPkywQn8XdIywpTlhGhXiccIjufR8jn+lPk7YQJgM4IjfTnmW+nZ/j7+XSzp0fKEKmFmvyeMCK4mzPFfT9j8UI2jY55XEtqkaYSy3S/RaTmH4FgWEtY8r0pc/2dCeT4Rr1tOz9NkJX5FqCPHACfF6aTjy+LsSphWui5RJ0oj6PMIbeXtkl4hOP4PxTKYTug0X01wRi+x6vRn/VgvVsXT+BAajPmExbp5BC99FHBUDBdhTvZJgrDGNJj+agRP/bEm2H478L4m3PPRhGmUx+JD/0gLnsM6sXxPSzndHQnTBH8j9MimEhZ6U3vGRf2Ua5Pg7OcmyumiPqTdsDaB/aMe3yA0hItJ7PppYjnsSWhcnyQs0rb6OaxSHxNhp0btrtvE/CvWkVaXQ72fQr/uo5w4/fJXwjz+twledXMLUwtOFeIw9wjgYgujICdliqLNOPVzBzDcwu66jkXhvWGzzOy2rG3JA1ktcDeLHQhDrtLUwH55q4x5xMJW01OztqPNyb02JV0B7Acc0+mOAsDMKm7a6FTaamThOI7jNIcsF7gdx3GcgpDZNNSQIUOsq6srq+ydNmfKlCmLLPzwrOW4tp1mkpW2M3MWXV1dTJ5c7fd1jtM3JDVz22ePuLadZpKVtn0aynEcx6mJOwvHcRynJu4sHMdxnJq4s8ghXSfcnLUJjtNUXOPFw52F4ziOUxN3Fo7jOE5N3Fk4juM4NXFn4TiO49TEnYXjOI5Tk3Z762yh8R0iTrvjGi8uPrJwHMdxauLOIgdU6m11nXCz98KctqGaxp3ikDtnMW6XcYzbZVzWZrScnipOyXGU4nglK6ZOimhzmtSj8Xritjt51UnunIXjOI6TP3K3wD368NFZm+AUgCLqpIg2O60nrzpxZ+EUkiLqpIg2O60nrzrJnbN4bdFrAAwcMjBjS/JJJ8/lJimiTopoc6txfedXJ6mtWUhaU9Ijkh6TNF3Sqb1JZ/wB4xl/wPi0zHLalFbpJC1dg2vbqY+86iTNkcUbwMfNbJmkAcD9km41s4dTzKPj8J5W5rium4jruzikNrKwwLJ4OCB+LK30ndp4xUsf13V+8N8eZUuqaxaS+gFTgJHAL8zsr2XhY4GxACNGjEgz60Liwi8GtXQd47i2K+Aabx9S/Z2Fmf3LzEYDmwDbS9q6LPwSMxtjZmOGDh2aZtYdh/eyWkctXcc4ru2UcY3ni6bshjKzlyXdA+wOTGvk2jFfH9MMk3JDSfxzztorY0uKTRY66Yuuof21naTrhJtd470krzpJzVlIGgq8FSvUWsAngB81ms7Wn1+l0+b0QKdWylbpJC1dg2u7t3SaxvOqkzRHFhsCV8T53dWA8WZ2U6OJLJm7BIDBwwenaFp704lD9RbqJBVdg2u7L3SSxvOqkzR3Q/3NzLYxs/9nZlub2Wm9SWfCoROYcOiEtMzKDeVi7yTxN4NW6SQtXUP7aruEazwd8qoTf5Gg4ziOUxN3Fo7jOE5N3Fk4juM4NXFn4TiO49Qkd2+d3eG4HbI2wSkARdRJEW12Wk9edZI7ZzHq06OyNqFp+O6Q9CiiTopoc6O4xvtOXnWSu2moRTMXsWjmoqzNcHJOEXVSRJud1pNXneTOWdx05E3cdGSvfvPkdBBF1EkRbXZaT151kjtn4TiO4+QPdxaO4zhOTdxZtBnJBUZfbHTakZKu/RXmrSV3u6HaiayE7BXIaRWu8c4hd85i55N2ztoEpwAUUSdFtNlpPXnVSe6cxea7bZ61CW1Fu/6zpSLqpIg2F4F2+38XedVJ7tYsFkxdwIKpC7I2w8k5RdRJEW12Wk9edZI7Z3Hbsbdx27G3ZW1G29Fuc7xF1EkRbS4K7aTvvOokNWchabikeyQ9Lmm6pGPSSttJh3aqUK3CdV0cfHdUc0lzzWIFcJyZPSppbWCKpDvM7PEU88g1SaG20xxqh9Pxuk7iGu9c0vy3qvPN7NH4/RVgBrBxWukXDe/htAeu6+q4xjuLpqxZSOoCtgH+2oz0HScLXNdOJ5P61llJg4A/AMea2dKysLHAWIARI0ZUvH7XM3ZN2ySnDWm1TnrSdQx3bTupkFedpOosJA0gVKirzOyP5eFmdglwCcCYMWOsUhrDPzI8TZOcNqWVOqmla3BtO+mRV52kuRtKwGXADDP7WW/TmfvgXOY+ODcts5w2pVU6SUvX4Np26iOvOklzzeI/gEOBj0uaGj97NprIXSfexV0n3pWiWU470kKdpKJrcG079ZFXnaQ2DWVm9wNKKz2nObTr6z+aheu6eLTb6z/yQu5+we20Bt/26LQzru/0cWfhOI7j1MSdheM4jlOT3L2ifPdzd8/ahIbxIW/rKaJOimhzEtd5a8irTnLnLIaNHpa1CR1FURe8i6iTItrcDhRtwTuvOsndNNTsO2cz+87ZWZvh5Jwi6qSINjutJ686yd3IYuLpE4H8/rcoJx8UUSdFtNlpPXnVSe5GFo7jOE7+cGfRB4q+4Fd0+53mU+R/KFRk2/NI7qahikBSgC5Gp11xnTtJfGThAN4YOO2N67vv5G5ksffFe2dtglMAiqiTItrstJ686iR3zmLIqCFZm+AUgCLqpIg2O60nrzrJ3TTUzBtnMvPGmVmbURUfzuaDvOukEkWy2XWeHXnVSe5GFg+d/RAAoz49KmNLOo8i/Zq7iDopos3tRFH0nVed5G5k4TiO4+SP3I0s8ooPy51OwHXuVCPN/8F9uaTnJU1LK03HyQOubcdJdxpqHJDPd+s6Tt8Yh2vb6XDS/B/cEyV19TWd/X+7f9+NSRkfmuePVuqknbWdxHWeD/Kqk5YucEsaK2mypMkvvPBCxTiDhw9m8PDBrTTLKaMIjUbedOLaLg5513deddJSZ2Fml5jZGDMbM3To0Ipxpl07jWnX+tRw1uS9QuVNJ67tYpFnfedVJ7nbDTX5l5MB2PrzW2dsiZNniqiTItrstJ686sR/Z+E4juPUJM2ts78DHgJGSZon6Stppe1kg/8/gIBruz1xfTdGmruhDk4rLcfJE65tx/FpKKcOvPfltDOu7/rI3QL3gdcdmLUJ3cST95eOdSp50Emj5M1m13k+yZtOSuTOWQwcMjBrE7rhvY58kjed1EOebXad54e86iR3zmLquKkAjD58dMaWOEny1gstok6KaHOnkCd951Un7iwi3rMqFnmtUD2RB5td5/knDzqphC9wO47jODVxZ+E4juPUxJ2F0zA+leG0M67vyrizwMXRG/zXr8XDn1f9eFmtiswsk4zHjBljkydPXuX8W6+9BcCAgQNaZosLo29ksXuklk4kTTGzMa20qUSetJ3Edd47Wq3vvGo7d7uhsqpITrEook6KaLPTevKqk9xNQ026cBKTLpzU9Hy8l5UeWZRlq3SSJlnY7NOFfafV5ZdXbeduZDF9/HQAtvvGdk3Lo/TwvRIVl1boJG1abXNS36714pBXbeduZNFMvJfVPLxs84U/i3Tx8uwwZ+G0Hq9kTrvSaR2k3E1DpUXXCTd328XQSQ81Syq9Y6d0Lut37rQj5WXrOm8e1d4fVd7WtCtt5SzcQTidgmvdaTWp/s5C0u7AeUA/4FIzO6ta3Gp70fuCV5j806oeWJp70RvRNTRH2+W41vNHEbXdCGn+D+5+wC+APYAtgYMlbZlW+tUozRt65SkeRditk5WuK+FaLw5F0HajpDaykLQDcIqZfSoefxfAzM6sFL+n3le7FK6THnPO2qvb/HyteeK0el+N6hoaG1m41p1yPbdK242S5prFxsDcxPE84EPJCJLGAmPj4TJJM2ukOQRYlJqFvSNrGzo9f4Ah+tFKG/Sj7n+rsGlKedfUNfRK2+VkXc5Z5t+peUNC20k9t0jbDdHSBW4zuwS4pN74kiZn9X6fvNjQ6fnnxYZaNKrtcrK+xyzz79S885B/I6T5O4tngeGJ403iOccpMq5rxyFdZzEJ2ELSZpJWBw4CbkgxfcfJAte145DiNJSZrZB0NPBnwhbDy81seh+T7fWwPkWytqHT84cMbWiSriuRdTlnmX+n5p2H/Osms/9n4TiO4xQHfzeU4ziOUxN3Fo7jOE5NcuUsJK0n6Q5J/4x/39lD3HUkzZN0QattkDRa0kOSpkv6m6TPp5Dv7pJmSpol6YQK4WtIujaG/1VSV1/zbDD//5b0eLzfuySlvte7lg2JeJ+VZJIKseWwEllrPQudZ6nxLPXdNro2s9x8gB8DJ8TvJwA/6iHuecDVwAWttgF4D7BF/L4RMB9Ytw959gOeBDYHVgceA7Ysi/MN4KL4/SDg2hTvuZ78PwYMjN+/nmb+9doQ460NTAQeBsZkrdlm6iwRN3Wtt1rnWWo8S323k65zNbIA9gWuiN+vAParFEnSB4ENgNuzsMHMnjCzf8bvzwHPA0P7kOf2wCwzm21mbwLXRDuq2XUdsKsk9SHPhvI3s3vM7LV4+DDh9wZpUk8ZAPwA+BGwPOX8W03WWm+1zrPUeJb6bhtd581ZbGBm8+P3BYRK0g1JqwFnA8dnZUOZPdsTegxP9iHPSq+U2LhaHDNbASwB1u9Dno3mn+QrwK0p5V23DZK2BYabWTu8UClrrbda51lqPEt9t42uW/7/LCTdCQyrEPS95IGZmaRK+3q/AdxiZvN62+lIwYZSOhsCvwUOM7O3e2VMwZD0RWAM8NEW57sa8DPg8Fbm2xey1rrrvHFare8i6brlzsLMdqsWJmmhpA3NbH4U6PMVou0A7CTpG8AgYHVJy8ys6sJRE2xA0jrAzcD3zOzhevOuQj2vlCjFmSepPzAYWNzHfBvJH0m7ERqaj5rZGynlXa8NawNbA/fGhnMYcIOkfcysuf88opdkrfWc6TxLjWep7/bRddaLJskP8BO6L7r9uEb8w0l/gbumDYTh+F3AsSnl2R+YDWzGykWwrcrifJPui3/jU7znevLfhjAFsUWTnn1NG8ri30tOFwLT0llZ/FS13mqdZ6nxLPXdTrrO3ICyglo/ivOfwJ3AevH8GMJ/KCuP3wxnUdMG4IvAW8DUxGd0H/PdE3giCvZ78dxpwD7x+5rA74FZwCPA5infd6387wQWJu73hiY8/x5tKIub20qVls7K4qftLFqu8yw1nqW+20XX/roPx3EcpyZ52w3lOI7j5BB3Fo7jOE5N3Fk4juM4NXFn4TiO49TEnYXjOI5TE3cWjuM4Tk3cWTiO4zg1+f/EP37v9b12PgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, you can specify affine or symmetric schemes while initializing the Observer. Note that not all observers support both schemes."
      ],
      "metadata": {
        "id": "EubEAsxRboDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
        "  obs = MovingAverageMinMaxObserver(qscheme=qscheme)\n",
        "  for x in inputs: obs(x)\n",
        "  print(f\"Qscheme: {qscheme} | {obs.calculate_qparams()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRTf8TUVbRBb",
        "outputId": "cf92cfb5-3e11-4513-e98b-6e4933a70711"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qscheme: torch.per_tensor_affine | (tensor([0.0093]), tensor([194], dtype=torch.int32))\n",
            "Qscheme: torch.per_tensor_symmetric | (tensor([0.0142]), tensor([128]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For weights quantization, symmetric-per-channel quantization provides better accuracies; per-tensor quantization performs poorly, possibly due to high variance in conv weights across channels from batchnorm folding."
      ],
      "metadata": {
        "id": "C8fZTh9ZcXv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.quantization.observer import MovingAveragePerChannelMinMaxObserver\n",
        "obs = MovingAveragePerChannelMinMaxObserver(ch_axis=0)  # calculate qparams for all `C` channels separately\n",
        "for x in inputs: \n",
        "  obs(x)\n",
        "print(obs.calculate_qparams())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usUkkoJcbzl_",
        "outputId": "b08c1a60-79dd-41a7-f3a7-deabf8258924"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.0074, 0.0052, 0.0071]), tensor([199, 147, 255], dtype=torch.int32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_qconfig = torch.quantization.QConfig(\n",
        "  activation=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_affine),\n",
        "  weight=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.qint8)\n",
        ")\n",
        "print(my_qconfig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zN_3aAUcrM7",
        "outputId": "710122cc-52ee-42b9-ec3b-c2cf76c98d03"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, qscheme=torch.per_tensor_affine){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, qscheme=torch.qint8){})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-Training Dynamic/Weight-only Quantization\n",
        "\n",
        "\n",
        "Here the model’s weights are pre-quantized; the activations are quantized on-the-fly (“dynamic”) during inference. The simplest of all approaches, it has a one line API call in torch.quantization.quantize_dynamic. Currently only Linear and Recurrent (LSTM, GRU, RNN) layers are supported for dynamic quantization.\n",
        "\n"
      ],
      "metadata": {
        "id": "hdVp4KSIeOAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# toy model\n",
        "m = nn.Sequential(\n",
        "  nn.Conv2d(2, 64, (8,)),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(16,10),\n",
        "  nn.LSTM(10, 10))\n",
        "\n",
        "m.eval()\n",
        "\n",
        "## EAGER MODE\n",
        "from torch.quantization import quantize_dynamic\n",
        "model_quantized = quantize_dynamic(\n",
        "    model=m, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "m7AmuA6fc5Sv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FX MODE\n",
        "from torch.quantization import quantize_fx\n",
        "qconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig}  # An empty key denotes the default applied to all modules\n",
        "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict, example_inputs=torch.rand(1, 3, 224, 224))\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ],
      "metadata": {
        "id": "ZAZyAoO_ewlC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Post-Training Static Quantization (PTQ)\n",
        "\n",
        "Module fusion combines multiple sequential modules (eg: [Conv2d, BatchNorm, ReLU]) into one. Fusing modules means the compiler needs to only run one kernel instead of many; this speeds things up and improves accuracy by reducing quantization error.\n",
        "\n",
        "(+) Static quantization has faster inference than dynamic quantization because it eliminates the float<->int conversion costs between layers.\n",
        "\n",
        "(-) Static quantized models may need regular re-calibration to stay robust against distribution-drift.\n"
      ],
      "metadata": {
        "id": "L8jFDxiqgU06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import copy\n",
        "\n",
        "backend = \"fbgemm\"  # running on a x86 CPU. Use \"qnnpack\" if running on ARM.\n",
        "\n",
        "model = nn.Sequential(\n",
        "     nn.Conv2d(2,64,3),\n",
        "     nn.ReLU(),\n",
        "     nn.Conv2d(64, 128, 3),\n",
        "     nn.ReLU()\n",
        ")\n",
        "\n",
        "## EAGER MODE\n",
        "m = copy.deepcopy(model)\n",
        "m.eval()\n",
        "\"\"\"Fuse\n",
        "- Inplace fusion replaces the first module in the sequence with the fused module, and the rest with identity modules\n",
        "\"\"\"\n",
        "torch.quantization.fuse_modules(m, ['0','1'], inplace=True) # fuse first Conv-ReLU pair\n",
        "torch.quantization.fuse_modules(m, ['2','3'], inplace=True) # fuse second Conv-ReLU pair\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTYne8Ile5BN",
        "outputId": "421c7829-e3ff-4514-b68e-cc3684083d4f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): ConvReLU2d(\n",
              "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (1): Identity()\n",
              "  (2): ConvReLU2d(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (3): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Insert stubs\"\"\"\n",
        "m = nn.Sequential(torch.quantization.QuantStub(), \n",
        "                  *m, \n",
        "                  torch.quantization.DeQuantStub())\n",
        "\n",
        "\"\"\"Prepare\"\"\"\n",
        "m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
        "torch.quantization.prepare(m, inplace=True)\n",
        "\n",
        "\"\"\"Calibrate\n",
        "- This example uses random data for convenience. Use representative (validation) data instead.\n",
        "\"\"\"\n",
        "with torch.inference_mode():\n",
        "  for _ in range(10):\n",
        "    x = torch.rand(1,2, 28, 28)\n",
        "    m(x)\n",
        "    \n",
        "\"\"\"Convert\"\"\"\n",
        "torch.quantization.convert(m, inplace=True)\n",
        "\n",
        "\"\"\"Check\"\"\"\n",
        "print(m[1].weight().element_size()) # 1 byte instead of 4 bytes for FP32\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGpM3wyIhFhA",
        "outputId": "9f20c9ac-4610-427e-a9c6-dd962402ceb6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## FX GRAPH\n",
        "from torch.quantization import quantize_fx\n",
        "m = copy.deepcopy(model)\n",
        "m.eval()\n",
        "qconfig_dict = {\"\": torch.quantization.get_default_qconfig(backend)}\n",
        "# Prepare\n",
        "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict,example_inputs=torch.rand(1, 3, 224, 224))\n",
        "# Calibrate - Use representative (validation) data.\n",
        "with torch.inference_mode():\n",
        "  for _ in range(10):\n",
        "    x = torch.rand(1,2,28, 28)\n",
        "    model_prepared(x)\n",
        "# quantize\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ],
      "metadata": {
        "id": "tEGSINaihihG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization-aware Training (QAT)\n"
      ],
      "metadata": {
        "id": "tgt7V0oyh825"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "backend = \"fbgemm\"  # running on a x86 CPU. Use \"qnnpack\" if running on ARM.\n",
        "\n",
        "m = nn.Sequential(\n",
        "     nn.Conv2d(2,64,8),\n",
        "     nn.ReLU(),\n",
        "     nn.Conv2d(64, 128, 8),\n",
        "     nn.ReLU()\n",
        ")\n",
        "\n",
        "\"\"\"Fuse\"\"\"\n",
        "torch.quantization.fuse_modules(m, ['0','1'], inplace=True) # fuse first Conv-ReLU pair\n",
        "torch.quantization.fuse_modules(m, ['2','3'], inplace=True) # fuse second Conv-ReLU pair\n",
        "\n",
        "\"\"\"Insert stubs\"\"\"\n",
        "m = nn.Sequential(torch.quantization.QuantStub(), \n",
        "                  *m, \n",
        "                  torch.quantization.DeQuantStub())"
      ],
      "metadata": {
        "id": "GR4-gGJjhlYy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Prepare\"\"\"\n",
        "m.train()\n",
        "m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
        "torch.quantization.prepare_qat(m, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLQ-LMDJiUXq",
        "outputId": "1ca8e6f8-6448-4345-b030-80287f58cc84"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): QuantStub(\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (1): ConvReLU2d(\n",
              "    2, 64, kernel_size=(8, 8), stride=(1, 1)\n",
              "    (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (2): Identity()\n",
              "  (3): ConvReLU2d(\n",
              "    64, 128, kernel_size=(8, 8), stride=(1, 1)\n",
              "    (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "  )\n",
              "  (4): Identity()\n",
              "  (5): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Training Loop\"\"\"\n",
        "n_epochs = 10\n",
        "opt = torch.optim.SGD(m.parameters(), lr=0.1)\n",
        "loss_fn = lambda out, tgt: torch.pow(tgt-out, 2).mean()\n",
        "for epoch in range(n_epochs):\n",
        "  x = torch.rand(10,2,24,24)\n",
        "  out = m(x)\n",
        "  loss = loss_fn(out, torch.rand_like(out))\n",
        "  opt.zero_grad()\n",
        "  loss.backward()\n",
        "  opt.step()"
      ],
      "metadata": {
        "id": "aXmlQGy2iWDg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Convert\"\"\"\n",
        "m.eval()\n",
        "torch.quantization.convert(m, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgbNOsTKiX-v",
        "outputId": "f3f91a02-d5dd-4d05-abc4-84fb85026135"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "  (1): QuantizedConvReLU2d(2, 64, kernel_size=(8, 8), stride=(1, 1), scale=0.012718189507722855, zero_point=0)\n",
              "  (2): Identity()\n",
              "  (3): QuantizedConvReLU2d(64, 128, kernel_size=(8, 8), stride=(1, 1), scale=0.00597846694290638, zero_point=0)\n",
              "  (4): Identity()\n",
              "  (5): DeQuantize()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SENSITIVITY ANALYSIS"
      ],
      "metadata": {
        "id": "GsfBA0wwino6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for quantized_layer, _ in model.named_modules():\n",
        "  print(\"Only quantizing layer: \", quantized_layer)\n",
        "\n",
        "  # The module_name key allows module-specific qconfigs. \n",
        "  qconfig_dict = {\"\": None, \n",
        "  \"module_name\":[(quantized_layer, torch.quantization.get_default_qconfig(backend))]}\n",
        "\n",
        "  model_prepared = quantize_fx.prepare_fx(model, qconfig_dict,example_inputs=torch.rand(1, 3, 224, 224))\n",
        "  # calibrate\n",
        "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
        "  # evaluate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tROZtf8yiZXc",
        "outputId": "62b02382-8875-4c12-93b1-d31c6df7cac9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only quantizing layer:  \n",
            "Only quantizing layer:  0\n",
            "Only quantizing layer:  1\n",
            "Only quantizing layer:  2\n",
            "Only quantizing layer:  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/ao/quantization/observer.py:1204: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/assets/images/quantization-practice/compare_output_ns.png"
      ],
      "metadata": {
        "id": "oO0qpMDtoug1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #extract from https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html\n",
        "# import torch.quantization._numeric_suite as ns\n",
        "\n",
        "# def SQNR(x, y):\n",
        "#     # Higher is better\n",
        "#     Ps = torch.norm(x)\n",
        "#     Pn = torch.norm(x-y)\n",
        "#     return 20*torch.log10(Ps/Pn)\n",
        "\n",
        "# wt_compare_dict = ns.compare_weights(fp32_model.state_dict(), int8_model.state_dict())\n",
        "# for key in wt_compare_dict:\n",
        "#     print(key, compute_error(wt_compare_dict[key]['float'], wt_compare_dict[key]['quantized'].dequantize()))\n",
        "\n",
        "# act_compare_dict = ns.compare_model_outputs(fp32_model, int8_model, input_data)\n",
        "# for key in act_compare_dict:\n",
        "#     print(key, compute_error(act_compare_dict[key]['float'][0], act_compare_dict[key]['quantized'][0].dequantize()))\n"
      ],
      "metadata": {
        "id": "FW_3MCwoipKB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}