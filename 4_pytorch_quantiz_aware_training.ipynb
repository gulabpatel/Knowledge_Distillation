{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulabpatel/Knowledge_Distillation/blob/main/4_pytorch_quantiz_aware_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "# from resnet import resnet18"
      ],
      "metadata": {
        "id": "l75KJnqt6CvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet18 = torchvision.models.resnet18()"
      ],
      "metadata": {
        "id": "mFiFGfEe6M7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)"
      ],
      "metadata": {
        "id": "4xgIw7fw6U_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform) \n",
        "    # We will use test set for validation and test in this project.\n",
        "    # Do not use test set for validation in practice!\n",
        "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
        "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset=train_set, batch_size=train_batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        dataset=test_set, batch_size=eval_batch_size,\n",
        "        sampler=test_sampler, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "BjZoWKYq6Zvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "76ac8de7b0a34314b3d2c378b6bf9993",
            "90bb4602592545bfbf1d303ec3d85ea2",
            "2237679cd7b84081915dae3a4d213abb",
            "c8b54718b1ae4ef182f246d293aec467",
            "21cf2caa39574c5190dd272827646cf4",
            "abc76eab53324b0a9b8394456818f17e",
            "0175ffd26dec49cdaa48201c7bc981b4",
            "530d4c29b62040b287ac44db22f898f7",
            "cbaab8e4a0fb429c9463b55cdcf49ba9",
            "727b7d968a14443782d8080b4e6b1425",
            "66acbd495b7248b9ae85e252f79bfe22"
          ]
        },
        "id": "HZfYcDDA6gyj",
        "outputId": "51bc021e-c6b1-4a0d-d48a-b91b399950e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76ac8de7b0a34314b3d2c378b6bf9993"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7fdf5ab49520>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7fdf5ab494f0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "        # statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "    return eval_loss, eval_accuracy"
      ],
      "metadata": {
        "id": "IqUzTHMn6sr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=200):\n",
        "\n",
        "    # The training configurations were not carefully selected.\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "    print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(-1, eval_loss, eval_accuracy))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "F7zRzOSJ62a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        _ = model(inputs)"
      ],
      "metadata": {
        "id": "6VY0Yg_n66gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 3, 32, 32),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave"
      ],
      "metadata": {
        "id": "PYt3hwUh7MSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)"
      ],
      "metadata": {
        "id": "dE3ZoOGb7OUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Uo7pfHYi7Qzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)"
      ],
      "metadata": {
        "id": "_vtkl13p7TEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_torchscript_model(model_filepath, device):\n",
        "\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XHBrBndH7WIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes=10):\n",
        "\n",
        "    # The number of channels in ResNet18 is divisible by 8.\n",
        "    # This is required for fast GEMM integer matrix multiplication.\n",
        "    # model = torchvision.models.resnet18(pretrained=False)\n",
        "    # model = resnet18(num_classes=num_classes, pretrained=False)\n",
        "    model = torchvision.models.resnet18(num_classes=num_classes, pretrained=False)\n",
        "\n",
        "    # We would use the pretrained ResNet18 as a feature extractor.\n",
        "    # for param in model.parameters():\n",
        "    #     param.requires_grad = False\n",
        "    \n",
        "    # Modify the last FC layer\n",
        "    # num_features = model.fc.in_features\n",
        "    # model.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "HOff52YC7Yu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedResNet18(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "        super(QuantizedResNet18, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "swVs7Cjm7joA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
        "\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        y1 = model_1(x).detach().cpu().numpy()\n",
        "        y2 = model_2(x).detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "7Vwc3gIq73r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    random_seed = 0\n",
        "    num_classes = 10\n",
        "    cuda_device = torch.device(\"cuda:0\")\n",
        "    cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "    model_dir = \"saved_models\"\n",
        "    model_filename = \"resnet18_cifar10.pt\"\n",
        "    quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
        "\n",
        "    set_random_seeds(random_seed=random_seed)\n",
        "\n",
        "    # Create an untrained model.\n",
        "    model = create_model(num_classes=num_classes)\n",
        "\n",
        "    train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
        "    \n",
        "    # Train model.\n",
        "    print(\"Training Model...\")\n",
        "    model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-1, num_epochs=5)\n",
        "    # Save model.\n",
        "    save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
        "    # Load a pretrained model.\n",
        "    model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
        "    # Move the model to CPU since static quantization does not support CUDA currently.\n",
        "    model.to(cpu_device)\n",
        "    # Make a copy of the model for layer fusion\n",
        "    fused_model = copy.deepcopy(model)\n",
        "\n",
        "    model.train()\n",
        "    # The model has to be switched to training mode before any layer fusion.\n",
        "    # Otherwise the quantization aware training will not work correctly.\n",
        "    # fused_model.train()\n",
        "    fused_model.eval()\n",
        "\n",
        "    # Fuse the model in place rather manually.\n",
        "    fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
        "    for module_name, module in fused_model.named_children():\n",
        "        if \"layer\" in module_name:\n",
        "            for basic_block_name, basic_block in module.named_children():\n",
        "                torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
        "                for sub_block_name, sub_block in basic_block.named_children():\n",
        "                    if sub_block_name == \"downsample\":\n",
        "                        torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
        "\n",
        "    # Print FP32 model.\n",
        "    print(model)\n",
        "    # Print fused model.\n",
        "    print(fused_model)\n",
        "\n",
        "    # Model and fused model should be equivalent.\n",
        "    model.eval()\n",
        "    fused_model.eval()\n",
        "    assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n",
        "\n",
        "    # Prepare the model for quantization aware training. This inserts observers in\n",
        "    # the model that will observe activation tensors during calibration.\n",
        "    quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "    # Using un-fused model will fail.\n",
        "    # Because there is no quantized layer implementation for a single batch normalization layer.\n",
        "    # quantized_model = QuantizedResNet18(model_fp32=model)\n",
        "    # Select quantization schemes from \n",
        "    # https://pytorch.org/docs/stable/quantization-support.html\n",
        "    quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "    # Custom quantization configurations\n",
        "    # quantization_config = torch.quantization.default_qconfig\n",
        "    # quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
        "\n",
        "    quantized_model.qconfig = quantization_config\n",
        "    \n",
        "    # Print quantization configurations\n",
        "    print(quantized_model.qconfig)\n",
        "\n",
        "    # https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#prepare_qat\n",
        "    torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
        "\n",
        "    # # Use training data for calibration.\n",
        "    print(\"Training QAT Model...\")\n",
        "    quantized_model.train()\n",
        "    train_model(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=10)\n",
        "    quantized_model.to(cpu_device)\n",
        "\n",
        "    # Using high-level static quantization wrapper\n",
        "    # The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
        "    # quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
        "\n",
        "    quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "\n",
        "    quantized_model.eval()\n",
        "\n",
        "    # Print quantized model.\n",
        "    print(quantized_model)\n",
        "\n",
        "    # Save quantized model.\n",
        "    save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
        "\n",
        "    # Load quantized model.\n",
        "    quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "    _, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "    _, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
        "\n",
        "    # Skip this assertion since the values might deviate a lot.\n",
        "    # assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "    print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "    print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "    fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
        "    \n",
        "    print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
        "    print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
        "    print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
        "    print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "FjnJ8xCh57Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6vJtH6hNXXBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "76ac8de7b0a34314b3d2c378b6bf9993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90bb4602592545bfbf1d303ec3d85ea2",
              "IPY_MODEL_2237679cd7b84081915dae3a4d213abb",
              "IPY_MODEL_c8b54718b1ae4ef182f246d293aec467"
            ],
            "layout": "IPY_MODEL_21cf2caa39574c5190dd272827646cf4"
          }
        },
        "90bb4602592545bfbf1d303ec3d85ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abc76eab53324b0a9b8394456818f17e",
            "placeholder": "​",
            "style": "IPY_MODEL_0175ffd26dec49cdaa48201c7bc981b4",
            "value": "100%"
          }
        },
        "2237679cd7b84081915dae3a4d213abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_530d4c29b62040b287ac44db22f898f7",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbaab8e4a0fb429c9463b55cdcf49ba9",
            "value": 170498071
          }
        },
        "c8b54718b1ae4ef182f246d293aec467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_727b7d968a14443782d8080b4e6b1425",
            "placeholder": "​",
            "style": "IPY_MODEL_66acbd495b7248b9ae85e252f79bfe22",
            "value": " 170498071/170498071 [00:13&lt;00:00, 13560516.20it/s]"
          }
        },
        "21cf2caa39574c5190dd272827646cf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abc76eab53324b0a9b8394456818f17e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0175ffd26dec49cdaa48201c7bc981b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "530d4c29b62040b287ac44db22f898f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbaab8e4a0fb429c9463b55cdcf49ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "727b7d968a14443782d8080b4e6b1425": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66acbd495b7248b9ae85e252f79bfe22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}