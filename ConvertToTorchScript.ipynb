{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNraaOT1qpFD9AjL1WysiDH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulabpatel/Knowledge_Distillation/blob/main/ConvertToTorchScript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchstat"
      ],
      "metadata": {
        "id": "3oDwOUj0iABX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtPejgyuaV-V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "model = torchvision.models.resnet18()\n",
        "torch.save(model.state_dict(), 'resnet18.pt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regarding the number of the parameters in PyTorch you can use:\n",
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZioci3yfL8R",
        "outputId": "4aca6f5b-3e4e-4aef-faa3-c741297e9a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11689512"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_size = os.path.getsize('/content/resnet18.pt')\n",
        "print(\"File Size is :\", file_size/1048576, \"MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XkLeDwbzCf1",
        "outputId": "740d170d-a7c7-4bfe-d71d-811db7a0949e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size is : 44.66516971588135 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/resnet18.pt\"))\n",
        "model.eval()\n",
        "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "# print(quantized_model)\n",
        "torch.save(quantized_model.state_dict(), 'quantized_modelv1.pt')\n",
        "\n",
        "file_size = os.path.getsize('/content/quantized_modelv1.pt')\n",
        "print(\"File Size is :\", file_size/1048576, \"MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtltRvehhLSF",
        "outputId": "485a93fe-3c79-4c0e-f4ab-1ff17d636d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size is : 43.20219898223877 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An example input you would normally provide to your model's forward() method.\n",
        "example = torch.rand(1, 3, 224, 224)\n",
        "\n",
        "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
        "traced_script_module = torch.jit.trace(model, example)\n",
        "\n",
        "output = traced_script_module(torch.ones(1, 3, 224, 224))\n",
        "# output"
      ],
      "metadata": {
        "id": "RZKKKrODbOVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# traced_script_module.save(\"traced_resnet_model.pt\")\n",
        "torch.save(traced_script_module.state_dict(), 'traced_resnet_model.pt')"
      ],
      "metadata": {
        "id": "IY9FgNXJzbmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_size = os.path.getsize('/content/traced_resnet_model.pt')\n",
        "print(\"File Size is :\", file_size/1048576, \"MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYvelz5Rzvxk",
        "outputId": "b10a289d-4821-481b-aa28-0e9b3412d323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size is : 44.666470527648926 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/traced_resnet_model.pt\"))\n",
        "model.eval()\n",
        "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "# print(quantized_model)\n",
        "torch.save(quantized_model.state_dict(), 'quantized_modelv2.pt')\n",
        "\n",
        "file_size = os.path.getsize('/content/quantized_modelv2.pt')\n",
        "print(\"File Size is :\", file_size/1048576, \"MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9Nss6kFjm5X",
        "outputId": "6b06319b-bccc-4113-ec23-4bce89b343c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size is : 43.20219898223877 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.ao.quantization import get_default_qconfig\n",
        "from torch.ao.quantization.quantize_fx import convert_fx, prepare_fx"
      ],
      "metadata": {
        "id": "GCSHmvAr0C8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from torch.ao.quantization import get_default_qconfig\n",
        "from torch.ao.quantization.quantize_fx import convert_fx, prepare_fx\n",
        "from torchvision.models import resnet50\n",
        "fp32_model = resnet50().eval()\n",
        "model = copy.deepcopy(fp32_model)\n",
        "# `qconfig` means quantization configuration, it specifies how should we\n",
        "# observe the activation and weight of an operator\n",
        "# `qconfig_dict`, specifies the `qconfig` for each operator in the model\n",
        "# we can specify `qconfig` for certain types of modules\n",
        "# we can specify `qconfig` for a specific submodule in the model\n",
        "# we can specify `qconfig` for some functioanl calls in the model\n",
        "# we can also set `qconfig` to None to skip quantization for some operators\n",
        "qconfig = get_default_qconfig(\"fbgemm\")\n",
        "qconfig_dict = {\"\": qconfig}\n",
        "# `prepare_fx` inserts observers in the model based on the configuration in `qconfig_dict`\n",
        "model_prepared = prepare_fx(model, qconfig_dict, torch.rand(1, 3, 224, 224))\n",
        "# calibration runs the model with some sample data, which allows observers to record the statistics of\n",
        "# the activation and weigths of the operators\n",
        "calibration_data = [torch.randn(1, 3, 224, 224) for _ in range(100)]\n",
        "for i in range(len(calibration_data)):\n",
        "   model_prepared(calibration_data[i])\n",
        "# `convert_fx` converts a calibrated model to a quantized model, this includes inserting\n",
        "# quantize, dequantize operators to the model and swap floating point operators with quantized operators\n",
        "model_quantized = convert_fx(copy.deepcopy(model_prepared))\n",
        "# benchmark\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "%timeit fp32_model(x)\n",
        "%timeit model_quantized(x)"
      ],
      "metadata": {
        "id": "lKaVYq6fzjbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36f43f0-8696-4927-89e5-259c5087b8e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/ao/quantization/fx/prepare.py:1530: UserWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "221 ms ± 6.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "112 ms ± 680 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModule(torch.nn.Module):\n",
        "    def __init__(self, N, M):\n",
        "        super(MyModule, self).__init__()\n",
        "        self.weight = torch.nn.Parameter(torch.rand(N, M))\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.sum() > 0:\n",
        "          output = self.weight.mv(input)\n",
        "        else:\n",
        "          output = self.weight + input\n",
        "        return output"
      ],
      "metadata": {
        "id": "-DnSPKFLbKdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModule(torch.nn.Module):\n",
        "    def __init__(self, N, M):\n",
        "        super(MyModule, self).__init__()\n",
        "        self.weight = torch.nn.Parameter(torch.rand(N, M))\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.sum() > 0:\n",
        "          output = self.weight.mv(input)\n",
        "        else:\n",
        "          output = self.weight + input\n",
        "        return output\n",
        "\n",
        "my_module = MyModule(10,20)\n",
        "sm = torch.jit.script(my_module)\n",
        "sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWx5vJEbmD0",
        "outputId": "1e43859a-1268-41f0-d3c4-44e81ed53f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecursiveScriptModule(original_name=MyModule)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sm.save(\"traced_resnet_modelv2.pt\")"
      ],
      "metadata": {
        "id": "liZfNSqdcKFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regarding the number of the parameters in PyTorch you can use:\n",
        "sum(p.numel() for p in sm.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7PH_gCpcx_l",
        "outputId": "f592bb01-94cf-405f-f62f-870d7cffb5df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_size = os.path.getsize('/content/traced_resnet_modelv2.pt')\n",
        "print(\"File Size is :\", file_size, \"bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr1XAnBB4alE",
        "outputId": "06b04f5c-ffb0-45c6-9f7a-d9da2363992e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size is : 2970 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"save model...\")              \n",
        "m = torch.jit.script(sm)\n",
        "with torch.no_grad() :\n",
        "    m.eval()\n",
        "    torch.save(m.state_dict(), 'freeze_model.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvz6IaAc6Z3c",
        "outputId": "b59cb209-b87a-44fe-c682-a612207d1e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "m.load_state_dict(torch.load(\"/content/freeze_model.pt\"))\n",
        "sm.eval()\n",
        "quantized_model = torch.quantization.quantize_dynamic(sm, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "# print(quantized_model)\n",
        "torch.save(quantized_model.state_dict(), 'quantized_modelv2.pt')"
      ],
      "metadata": {
        "id": "6X7_K0pdfu1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_size = os.path.getsize('/content/quantized_modelv2.pt')\n",
        "print(\"File Size is :\", file_size, \"bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RynHJceI2Q9b",
        "outputId": "6b80b518-0c6a-4485-875e-a8595b1ff3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size is : 1609 bytes\n"
          ]
        }
      ]
    }
  ]
}